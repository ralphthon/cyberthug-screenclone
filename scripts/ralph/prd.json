{
  "project": "RalphTon (ScreenClone)",
  "branchName": "feature/screenclone",
  "description": "RalphTon is a web application where users interact with Cloney ‚Äî a Live2D character powered by OpenWaifu (Open-LLM-VTuber) ‚Äî to clone websites from screenshots. The user drops a screenshot and chats with Cloney, who orchestrates ralph-image-analysis (ralph.sh --tool omx) as the backend engine. Ralph iteratively generates HTML/CSS/JS, renders via Puppeteer, and evaluates using a dual comparison system: (1) Vision-based verdict via GPT-5.3 Codex image understanding for qualitative service-level similarity scoring + specific actionable feedback, and (2) pixelmatch for quantitative pixel-diff visualization. The Vision verdict is the primary feedback signal driving the ralph loop ‚Äî it provides structured differences and suggestions that feed directly into the next iteration's prompt. Loop continues until Vision verdict score exceeds 90%. Cloney reports progress conversationally with emotion-driven TTS (Qwen3) and Live2D expressions (WaifuClaw 16 expressions). Each improving iteration auto-commits to GitHub.",
  "userStories": [
    {
      "id": "US-001",
      "title": "Vite + React + TypeScript + TailwindCSS project scaffold",
      "description": "Initialize the monorepo structure with a Vite-powered React 18 frontend and a separate Express backend directory. Frontend uses TypeScript strict mode and TailwindCSS with dark mode as default. Backend uses TypeScript with ts-node. Shared types in a common directory.",
      "acceptanceCriteria": [
        "npm run dev starts frontend at localhost:5173 without errors",
        "npm run server starts Express backend at localhost:3001 without errors",
        "Page renders 'RalphTon' heading with gradient text on dark background (#1e1b2e)",
        "TailwindCSS dark mode classes apply correctly (class strategy)",
        "TypeScript strict mode enabled, tsc --noEmit passes for both frontend and backend",
        "Directory structure: src/client/, src/server/, src/shared/types/",
        "package.json has scripts: dev (frontend), server (backend), dev:all (concurrently)",
        "ESLint + Prettier configured with consistent rules",
        "CORS configured on Express to allow localhost:5173"
      ],
      "priority": 1,
      "passes": true,
      "notes": "Use concurrently for dev:all script. TailwindCSS config: darkMode 'class', custom colors: primary #6366f1, surface #1e1b2e, card #2a2740.",
      "status": "passed"
    },
    {
      "id": "US-002",
      "title": "Drag-and-drop screenshot upload with preview thumbnails",
      "description": "Full-featured drop zone component that accepts PNG/JPG/WEBP screenshots via drag-and-drop or click-to-browse. Shows thumbnail previews with remove buttons. Validates file type and size. Stores files in component state and prepares FormData for upload.",
      "acceptanceCriteria": [
        "Drop zone visible on main page with dashed border, icon, and 'Drop screenshots here' text",
        "Accepts drag-and-drop: dragenter highlights border (primary color), dragleave reverts",
        "Click-to-browse opens file picker with accept='image/png,image/jpeg,image/webp'",
        "Rejects non-image files with toast error message",
        "Max file size 10MB per image, shows error for oversized files",
        "Max 5 images enforced ‚Äî drop zone shows '5/5' counter and disables further uploads",
        "Thumbnail grid renders below drop zone (5 columns) with aspect-ratio preserved",
        "Each thumbnail has X button in top-right corner to remove",
        "Removing an image updates counter and re-enables drop zone if was at max",
        "Files stored as File objects in React state (not uploaded yet)",
        "Empty state shows subtle placeholder boxes",
        "Typecheck passes"
      ],
      "priority": 2,
      "passes": true,
      "notes": "Use react-dropzone or native drag events. Object URLs for previews ‚Äî revoke on remove/unmount to prevent memory leaks.",
      "status": "passed"
    },
    {
      "id": "US-003",
      "title": "Clone session configuration form with validation",
      "description": "Form below the upload zone with project name, GitHub repo URL, GitHub token, max iterations, and target similarity score. Validates required fields. Saves last-used settings to localStorage. Start button triggers the clone session.",
      "acceptanceCriteria": [
        "Form renders in a card below upload zone with consistent dark theme styling",
        "Fields: project name (text, required), GitHub repo URL (text, optional), GitHub token (password, optional), max iterations (number, default 1000, min 1), target similarity (number, default 90, range 50-100)",
        "Start button disabled until: project name filled AND at least 1 image uploaded",
        "Start button shows 'üöÄ Start Cloning' with primary color background",
        "On submit: saves all field values to localStorage keyed by 'ralphton-config'",
        "On mount: loads saved values from localStorage and pre-fills form",
        "GitHub token field has show/hide toggle icon",
        "Validation errors shown inline below each field in red text",
        "When clone is running, form is disabled with opacity reduction",
        "Typecheck passes"
      ],
      "priority": 3,
      "passes": true,
      "notes": "Use react-hook-form or simple controlled components. localStorage key: 'ralphton-config'.",
      "status": "passed"
    },
    {
      "id": "US-004",
      "title": "Express backend with multipart image upload API",
      "description": "Express server with multer middleware for image uploads. POST /api/upload accepts up to 5 images, validates MIME type, stores in session-specific temp directory, returns file paths and metadata. Includes health check and CORS.",
      "acceptanceCriteria": [
        "GET /api/health returns { status: 'ok', version: '1.0.0' }",
        "POST /api/upload accepts multipart/form-data with field name 'screenshots'",
        "Multer configured: max 5 files, max 10MB each, filter for image/png, image/jpeg, image/webp only",
        "Files stored in /tmp/ralphton-{sessionId}/ with original filenames",
        "Returns JSON: { sessionId: string, files: Array<{ filename, path, size, mimetype }> }",
        "Session ID is UUID v4, generated per upload request",
        "Rejects non-image files with 400 and descriptive error message",
        "Rejects requests with no files with 400",
        "Rejects requests exceeding 5 files with 400",
        "Temp directories cleaned up after 24 hours via setInterval cleanup",
        "Express error handler returns consistent JSON error format: { error: string, code: string }",
        "Typecheck passes"
      ],
      "priority": 4,
      "passes": true,
      "notes": "Use multer with diskStorage. uuid package for session IDs. Cleanup interval: check /tmp/ralphton-* directories older than 24h.",
      "status": "passed"
    },
    {
      "id": "US-005",
      "title": "Vision API screenshot analyzer endpoint",
      "description": "POST /api/analyze sends uploaded screenshots to a vision-capable LLM (OpenAI GPT-4V or Claude 3.5) to extract structured UI description. Returns layout structure, color palette, component inventory, text content, and responsive breakpoints.",
      "acceptanceCriteria": [
        "POST /api/analyze accepts { sessionId: string, imageIndex?: number }",
        "Reads images from session temp directory",
        "Sends images as base64 to vision LLM with structured extraction prompt",
        "Returns JSON: { layout: { type, direction, sections[] }, colorPalette: { primary, secondary, background, text, accent }, components: Array<{ type, position, props }>, textContent: string[], fonts: string[], responsiveHints: string[] }",
        "Supports OPENAI_API_KEY env var for GPT-4V, ANTHROPIC_API_KEY for Claude",
        "Falls back to Claude if OpenAI unavailable, and vice versa",
        "Timeout: 60 seconds per analysis request",
        "Caches analysis result in session state to avoid re-analyzing same images",
        "Returns 404 if session ID not found, 500 if LLM call fails with retry info",
        "Typecheck passes"
      ],
      "priority": 5,
      "passes": true,
      "notes": "Vision prompt should extract: DOM structure estimate, CSS grid/flex layout, exact hex colors, font families, component types (nav, hero, card, footer, etc).",
      "status": "passed"
    },
    {
      "id": "US-006",
      "title": "Ralph loop process manager ‚Äî spawn and manage ralph.sh",
      "description": "Core backend service that spawns ralph.sh from ralph-image-analysis as a child process per clone session. Manages process lifecycle, captures stdout/stderr in ring buffer, watches progress.txt for changes, and emits structured events. Each session gets its own isolated ralph.sh process with --tool omx and the session's uploaded images.",
      "acceptanceCriteria": [
        "RalphProcessManager class with methods: start(sessionId, config), stop(sessionId), getStatus(sessionId), getOutput(sessionId, lastN)",
        "start() generates per-session prd.json from user config + analysis results, writes to session temp dir",
        "start() spawns: ralph.sh --tool omx --images-dir <session-images-dir> <maxIterations>",
        "Child process stdout/stderr captured in ring buffer (last 500 lines per session)",
        "fs.watch on progress.txt ‚Äî parses new entries and emits 'iteration-complete' events",
        "Detects iteration number from ralph.sh stdout pattern: 'Ralph Iteration N of M'",
        "Detects completion from '<promise>COMPLETE</promise>' in stdout",
        "Detects failure from process exit code != 0",
        "Session state machine: idle ‚Üí uploading ‚Üí analyzing ‚Üí cloning ‚Üí completed | failed",
        "Max 3 concurrent sessions (configurable via RALPH_MAX_SESSIONS env var)",
        "Graceful shutdown: SIGTERM to all child processes on server stop",
        "getStatus returns: { state, currentIteration, maxIterations, lastScore, startedAt, elapsedMs }",
        "EventEmitter pattern for state changes ‚Äî consumed by SSE endpoint",
        "Typecheck passes"
      ],
      "priority": 6,
      "passes": true,
      "notes": "ralph.sh lives at scripts/ralph/ralph.sh. prd.json template generated per-session with single story: 'Clone the uploaded screenshot'. child_process.spawn with {stdio: 'pipe', cwd: sessionDir}. Kill child on session abort.",
      "status": "passed"
    },
    {
      "id": "US-007",
      "title": "Puppeteer headless render and screenshot capture service",
      "description": "POST /api/render accepts HTML string, renders in headless Puppeteer, captures screenshot at specified viewport dimensions, returns base64 PNG. Used by the ralph loop to screenshot each iteration's generated code for visual comparison.",
      "acceptanceCriteria": [
        "POST /api/render accepts { html: string, width?: number, height?: number, waitMs?: number }",
        "Default viewport 1536x1024 (matches reference screenshot dimensions)",
        "Renders HTML in isolated Puppeteer page with no external network access (--disable-web-security off)",
        "Waits for: DOMContentLoaded + optional waitMs (default 1000ms) for CSS transitions",
        "Returns { screenshot: string (base64 PNG), width: number, height: number, renderTimeMs: number }",
        "Puppeteer browser instance shared across requests (singleton, lazy-init)",
        "Each render uses a new page (closed after screenshot to prevent memory leaks)",
        "Timeout per render: 30 seconds, returns 504 on timeout",
        "Rejects HTML larger than 1MB with 413",
        "Injects <base> tag to prevent external resource loading",
        "Typecheck passes"
      ],
      "priority": 7,
      "passes": true,
      "notes": "Use puppeteer-core + system Chromium or puppeteer with bundled Chromium. Singleton browser pattern: let browser = null; async function getBrowser() { if (!browser) browser = await puppeteer.launch(...); return browser; }",
      "status": "passed"
    },
    {
      "id": "US-008",
      "title": "Vision-based visual verdict engine (primary) + pixelmatch diff overlay (secondary)",
      "description": "Dual comparison system for evaluating clone quality. PRIMARY: Vision verdict via GPT-5.3 Codex image understanding ‚Äî sends original + generated screenshots to the vision model, returns structured verdict with score, categorical matches, specific differences, and actionable suggestions. This is the main feedback signal driving the ralph loop and what makes each iteration smarter. SECONDARY: pixelmatch pixel-diff for visual overlay in the UI only. The Vision verdict evaluates 'is this the same web service' rather than 'are these identical pixels'.",
      "acceptanceCriteria": [
        "POST /api/compare accepts { original: string (base64), generated: string (base64), mode?: 'vision' | 'pixel' | 'both' }",
        "Default mode: 'both' ‚Äî runs Vision verdict + pixelmatch in parallel (Promise.all)",
        "Vision verdict calls GPT-5.3 Codex with both images as base64 + structured JSON output prompt",
        "Vision verdict response schema: { score: number (0-100), layout_match: boolean, color_match: boolean, component_match: boolean, text_match: boolean, responsive_match: boolean, differences: string[] (specific issues found, max 10 items), suggestions: string[] (actionable code fixes, max 5 items), verdict: 'pass' | 'close' | 'fail', reasoning: string (1-2 sentence summary) }",
        "Vision prompt instructs: evaluate as 'same web service/product' not 'pixel-identical' ‚Äî judge layout structure, visual hierarchy, color palette coherence, component inventory completeness, text content accuracy, overall brand feel",
        "score interpretation: 0-30 fail (fundamentally different), 30-60 fail (structure recognizable but major issues), 60-80 close (similar but notable differences), 80-90 close (very similar, minor polish needed), 90+ pass (production-quality clone)",
        "Pixelmatch runs in parallel: returns { pixelScore: number, diffImage: string (base64 PNG red overlay), mismatchedPixels: number, totalPixels: number }",
        "Combined response: { vision: {verdict object}, pixel: {pixelmatch object}, primaryScore: number (= vision.score) }",
        "primaryScore (vision.score) is the loop control signal ‚Äî loop stops when >= targetScore",
        "differences[] and suggestions[] injected into progress.txt as structured feedback: '## Visual Verdict ‚Äî Iteration N\\nScore: X/100 (verdict)\\n### Differences\\n- diff1\\n- diff2\\n### Suggestions\\n- fix1\\n- fix2'",
        "This feedback is what ralph.sh reads on next iteration ‚Äî the agent sees exactly what to fix",
        "Caches verdict per (sessionId, iteration) ‚Äî avoids re-calling on status/reconnect",
        "Falls back to pixelmatch-only mode if Vision API fails (logs warning, uses pixel score as primaryScore)",
        "Rate limiting: max 1 Vision API call per 5 seconds per session",
        "Resizes generated image to match original dimensions before both comparisons (sharp)",
        "Returns 400 if images are invalid/corrupt base64",
        "Typecheck passes"
      ],
      "priority": 8,
      "passes": true,
      "notes": "This is the key innovation: ralph doesn't just know the score, it knows WHAT to fix and HOW. Vision prompt template: 'You are a senior frontend developer reviewing a website clone attempt. Image 1 is the ORIGINAL. Image 2 is the CLONE. Evaluate similarity as a web product (not pixels). Return JSON: {score, layout_match, color_match, component_match, text_match, responsive_match, differences[], suggestions[], verdict, reasoning}'. Use pixelmatch + pngjs for secondary diff image. sharp for resizing.",
      "status": "passed"
    },
    {
      "id": "US-009",
      "title": "Ralph loop orchestrator with SSE real-time streaming",
      "description": "POST /api/loop/start initiates a full clone session: upload ‚Üí analyze ‚Üí spawn ralph.sh ‚Üí stream progress via Server-Sent Events. Each iteration event includes score, screenshot, code preview, and diff image. GET /api/loop/:sessionId/status for polling. GET /api/loop/:sessionId/events for SSE stream.",
      "acceptanceCriteria": [
        "POST /api/loop/start accepts { sessionId: string, config: { projectName, maxIterations, targetScore, githubUrl?, githubToken? } }",
        "Validates sessionId exists and has uploaded images",
        "Calls /api/analyze internally, then starts RalphProcessManager",
        "GET /api/loop/:sessionId/events returns SSE stream (Content-Type: text/event-stream)",
        "SSE events emitted: iteration-start, iteration-complete, loop-complete, loop-error",
        "iteration-complete event data: { iteration, maxIterations, score, previousScore, improvement, screenshotBase64, codePreview (first 200 chars), diffImageBase64, commitUrl?, elapsedMs }",
        "loop-complete event data: { totalIterations, finalScore, totalElapsedMs, bestIteration }",
        "loop-error event data: { error, iteration, lastScore }",
        "SSE keepalive: sends comment every 15 seconds to prevent timeout",
        "GET /api/loop/:sessionId/status returns full session state (for reconnection/polling)",
        "POST /api/loop/:sessionId/stop sends SIGTERM to ralph.sh process",
        "Handles client disconnect: keeps ralph running but stops SSE stream",
        "Multiple clients can subscribe to same session's SSE stream",
        "Typecheck passes"
      ],
      "priority": 9,
      "passes": true,
      "notes": "SSE format: 'event: iteration-complete\\ndata: {json}\\n\\n'. Use EventEmitter from RalphProcessManager. res.write() for SSE, res.flush() if compression enabled. Set headers: Cache-Control no-cache, Connection keep-alive.",
      "status": "passed"
    },
    {
      "id": "US-010",
      "title": "Real-time iteration timeline UI with live updates",
      "description": "Main frontend view showing iteration progress as a vertical timeline of cards. Cards appear in real-time as SSE events arrive. Each card shows iteration number, thumbnail screenshot, similarity score with color coding, improvement delta, and feedback text. Clicking a card expands to show full screenshot + code.",
      "acceptanceCriteria": [
        "Timeline renders as vertical list, newest iteration at top",
        "Cards appear with slide-in animation as SSE events arrive (useEventSource hook)",
        "Each card shows: iteration # (large bold), thumbnail (120x80px), score (color: red <50, yellow 50-80, green >80), improvement delta (+/- from previous), feedback snippet (truncated to 1 line)",
        "Score color coding: text-red-400 for <50%, text-yellow-400 for 50-80%, text-green-400 for >80%",
        "Clicking card expands inline to show: full-size screenshot, code preview (syntax highlighted), diff image overlay, commit link if available",
        "Active/running iteration shows pulsing border animation",
        "Completed loop shows confetti/success banner at top",
        "Failed loop shows error banner with retry button",
        "Auto-scrolls to newest card unless user has scrolled up (intersection observer)",
        "Empty state: 'Waiting for first iteration...' with spinner",
        "Handles SSE reconnection automatically on disconnect (retry after 3s)",
        "Typecheck passes"
      ],
      "priority": 10,
      "passes": true,
      "notes": "Custom useEventSource hook wrapping EventSource API. Use framer-motion or CSS animations for card entrance. Intersection Observer for auto-scroll control.",
      "status": "passed"
    },
    {
      "id": "US-011",
      "title": "Side-by-side original vs clone comparison view with slider",
      "description": "Split comparison view: original screenshot (left) vs latest generated output (right). Interactive slider overlay that blends between the two. Toggle for pixel-diff red overlay mode. Similarity score bar between images.",
      "acceptanceCriteria": [
        "Split view layout: 50/50 grid with original (left) and generated (right)",
        "Both images rendered at equal size, maintaining aspect ratio",
        "Interactive slider: draggable vertical divider that reveals original/generated (CSS clip-path)",
        "Slider position shown as percentage label",
        "Three view modes via toggle buttons: Side-by-Side, Slider, Diff Overlay",
        "Diff Overlay mode: shows diff image (red pixels on transparent) overlaid on original",
        "Similarity score bar between images: gradient from red to green, filled to score %",
        "Score displayed numerically next to bar with decimal precision (e.g., 87.3%)",
        "Dropdown to select which iteration to compare (defaults to latest)",
        "Zoom: scroll wheel zooms both images synchronously",
        "Pan: click-drag pans both images synchronously when zoomed",
        "Keyboard: left/right arrow to switch iterations, space to toggle overlay",
        "Typecheck passes"
      ],
      "priority": 11,
      "passes": true,
      "notes": "Slider: use CSS clip-path: inset(0 {100-sliderPos}% 0 0) on overlay image. Synchronized zoom/pan via shared transform state.",
      "status": "passed"
    },
    {
      "id": "US-012",
      "title": "Live similarity score line chart",
      "description": "Recharts line chart showing similarity score progression over iterations. Updates in real-time as new iterations complete. Shows target threshold line, best score marker, and trend indicator.",
      "acceptanceCriteria": [
        "Line chart with X-axis = iteration number, Y-axis = similarity score (0-100%)",
        "Data points update live as SSE iteration-complete events arrive",
        "Horizontal dashed line at target score (default 90%) labeled 'Target'",
        "Line color changes: red segment <50%, yellow 50-80%, green >80%",
        "Best score point highlighted with larger dot + label",
        "Tooltip on hover: shows iteration #, score, improvement, elapsed time",
        "Y-axis always 0-100, X-axis auto-scales with iterations",
        "Chart height: 200px in sidebar, expandable to 400px on click",
        "Shows 'No data' placeholder before first iteration",
        "Smooth animation on new data point (animationDuration 300ms)",
        "Typecheck passes"
      ],
      "priority": 12,
      "passes": true,
      "notes": "Use recharts: LineChart, Line, XAxis, YAxis, Tooltip, ReferenceLine for target. CartesianGrid with strokeDasharray for subtle grid.",
      "status": "passed"
    },
    {
      "id": "US-013",
      "title": "GitHub auto-commit per improving iteration",
      "description": "When GitHub repo URL and token are configured, each iteration that improves the best score auto-commits the generated code to a dedicated branch. Commit message includes iteration number and score. Commit URL returned in SSE events.",
      "acceptanceCriteria": [
        "Creates branch 'ralph/clone-{projectName}-{timestamp}' on first improving iteration",
        "Commits generated HTML/CSS/JS files with message: 'ralph(#{iteration}): score {score}% (+{delta}%)'",
        "Only commits when current score > previous best score (no regression commits)",
        "Commit includes: index.html, styles.css, script.js, and README.md with score history",
        "README.md auto-generated with: project name, original screenshot (as reference), iteration log table",
        "Commit URL included in SSE iteration-complete event: commitUrl field",
        "Uses GitHub REST API via octokit ‚Äî creates tree + commit (not push, to handle fast-forward)",
        "On final iteration: creates summary commit with '[final]' prefix",
        "Handles GitHub API errors gracefully: logs warning, continues loop without blocking",
        "GitHub token validated on session start (GET /user), returns 401 if invalid",
        "Typecheck passes"
      ],
      "priority": 13,
      "passes": true,
      "notes": "Use @octokit/rest. createOrUpdateFileContents API for simplicity. Branch from default branch HEAD.",
      "status": "passed"
    },
    {
      "id": "US-014",
      "title": "Download final result as ZIP archive",
      "description": "Button to download the final generated website as a production-ready ZIP file. Includes all generated code, assets, README with build history, and original screenshots for reference.",
      "acceptanceCriteria": [
        "Download button appears in header bar when loop completes (state: completed)",
        "Button text: 'üì¶ Download ZIP' with file size estimate",
        "GET /api/loop/:sessionId/download returns application/zip",
        "ZIP contains: index.html (final generated), styles.css, script.js, README.md",
        "README.md includes: project name, final score, total iterations, original screenshot as embedded base64, score history table, timestamp",
        "ZIP also includes originals/ directory with uploaded reference screenshots",
        "ZIP also includes iterations/ directory with best 3 iteration screenshots",
        "ZIP file named: ralphton-{projectName}-{score}pct.zip",
        "Backend generates ZIP in memory using archiver or jszip",
        "Returns 404 if session not found, 409 if loop still running",
        "Typecheck passes"
      ],
      "priority": 14,
      "passes": true,
      "notes": "Use archiver package for streaming ZIP creation. Pipe directly to response for memory efficiency.",
      "status": "passed"
    },
    {
      "id": "US-015",
      "title": "OpenWaifu WebSocket bridge ‚Äî embed Live2D Cloney + OLV settings panel in frontend",
      "description": "Integrate OpenWaifu's WebSocket client into the React frontend. Right-side panel contains: (1) OLV Settings section at top ‚Äî server URL, LLM model selection, API key, TTS voice, persona toggle, connection test button. (2) Live2D canvas rendering WaifuClaw model in the middle. (3) Chat history with conversation bubbles. (4) Text input at bottom. Panel connects to OpenWaifu/OLV server via WebSocket.",
      "acceptanceCriteria": [
        "Right-side panel (30% width) with 4 sections: OLV Settings (top), Live2D canvas, chat history, text input",
        "OLV Settings section (collapsible with toggle):",
        "  - Server URL input (default ws://localhost:12393/ws, configurable)",
        "  - LLM Model dropdown (OpenAI GPT-4o, Claude 4 Sonnet, Gemini 3 Pro, Ollama local, custom)",
        "  - LLM API Key input (password field with show/hide toggle)",
        "  - LLM Base URL input (for custom endpoints)",
        "  - TTS Voice selector (Sohee default, with preview button)",
        "  - Persona toggle (enable/disable custom persona prompt)",
        "  - Connection test button ‚Äî pings OLV WebSocket, shows green/red status",
        "  - Save Settings button ‚Äî persists to localStorage",
        "  - Settings auto-loaded from localStorage on mount",
        "Live2D canvas renders WaifuClaw model via pixi-live2d-display or OpenWaifu's built-in renderer",
        "WebSocket connects using configured server URL from settings",
        "Handles WebSocket message types: text-input (send), audio-play-start (receive), display_text (receive), set-expression (receive)",
        "Chat history shows conversation bubbles: user (right, primary) and Cloney (left, card color)",
        "Cloney messages include emotion tags: [joy]‚Üíüòä, [sadness]‚Üíüò¢, [surprise]‚Üíüò≤, [neutral]‚Üíüòê",
        "Connection status indicator: green (connected), yellow (connecting), red (disconnected)",
        "Auto-reconnect on disconnect with exponential backoff (1s‚Üí30s max)",
        "Panel collapsible via toggle button (saves state to localStorage)",
        "When collapsed: floating Cloney avatar (64x64) in bottom-right with unread badge",
        "Typecheck passes"
      ],
      "priority": 15,
      "passes": true,
      "notes": "OLV settings map to conf.yaml fields: openai_compatible_llm.base_url, openai_compatible_llm.llm_api_key, openai_compatible_llm.model, qwen3_tts.voice. Settings saved to localStorage key: \"ralphton-olv-config\". On save, optionally POST to OLV /api/config to hot-reload without restart. Connection test: WebSocket open + close within 3s = success.",
      "status": "passed"
    },
    {
      "id": "US-016",
      "title": "Cloney conversation ‚Üí ScreenClone action bridge",
      "description": "Bridge layer that intercepts Cloney's conversation context and maps user intents to ScreenClone backend actions. When user tells Cloney to clone something, Cloney triggers the ralph loop. When screenshots are shared in chat, they're added to the session. Cloney's LLM persona is extended with tool-use instructions for ScreenClone operations.",
      "acceptanceCriteria": [
        "Intent detection: user messages containing 'clone', 'ÌÅ¥Î°†', 'copy', 'Î≥µÏÇ¨', 'make this', 'ÎßåÎì§Ïñ¥' trigger clone flow",
        "When clone intent detected + images uploaded: Cloney calls POST /api/loop/start via internal bridge",
        "When clone intent detected + no images: Cloney asks user to upload screenshots first",
        "Image paste in chat input: extracts image, adds to upload zone, Cloney acknowledges",
        "Cloney persona extended with ScreenClone tool instructions injected into OpenWaifu persona_prompt",
        "Tool instructions include: available commands (clone, stop, status, compare), how to interpret scores, how to report progress",
        "Bridge maintains session mapping: OpenWaifu session ‚Üî ScreenClone sessionId",
        "When ralph loop is running: Cloney intercepts SSE events and generates natural language responses",
        "SSE event ‚Üí Cloney response mapping: iteration-start ‚Üí 'ÏãúÏûëÌï†Í≤å~', iteration-complete ‚Üí score-based response, loop-complete ‚Üí celebration, loop-error ‚Üí error explanation",
        "User can say 'stop' or 'Î©àÏ∂∞' to trigger POST /api/loop/:sessionId/stop",
        "User can say 'status' or 'Ïñ¥ÎîîÍπåÏßÄ ÌñàÏñ¥' to get current iteration info",
        "Typecheck passes"
      ],
      "priority": 16,
      "passes": true,
      "notes": "Bridge is a React context provider that wraps both OpenWaifu WebSocket and ScreenClone SSE. Intent detection can be simple keyword matching (no NLP needed ‚Äî Cloney's LLM handles ambiguity).",
      "status": "passed"
    },
    {
      "id": "US-017",
      "title": "OMX orchestration ‚Äî ralph.sh integration with per-session prd.json",
      "description": "Backend generates a customized prd.json for each clone session based on the vision analysis results. The prd.json contains a single user story: 'Clone the uploaded screenshot to match the analyzed UI structure'. ralph.sh --tool omx reads this prd.json and the OMX coding agent iterates on code generation. The orchestrator monitors ralph.sh output, extracts generated files, and feeds them to the render + compare pipeline.",
      "acceptanceCriteria": [
        "Per-session prd.json generated with project name, branch name, and single user story",
        "User story acceptance criteria derived from vision analysis: layout matches, colors within delta, components present, text content matches",
        "ralph.sh spawned with CWD set to session workspace directory (contains generated code)",
        "Generated files (index.html, styles.css, script.js) extracted from session workspace after each iteration",
        "After each ralph.sh iteration: render generated HTML ‚Üí screenshot ‚Üí compare with original ‚Üí log score",
        "Score + diff image injected back into ralph.sh context via progress.txt update",
        "If ralph.sh generates broken HTML (render fails): log error in progress.txt, continue to next iteration",
        "Session workspace isolated: /tmp/ralphton-{sessionId}/workspace/ with git init",
        "ralph.sh environment: PATH includes omx, OPENAI_API_KEY set from server env",
        "Typecheck passes"
      ],
      "priority": 17,
      "passes": false,
      "notes": "Key insight: ralph.sh manages its own iteration loop. Our backend wraps it with: (1) prd.json generation, (2) render+compare after each iteration, (3) SSE event emission. The feedback loop is: ralph generates code ‚Üí we render+screenshot ‚Üí we compare ‚Üí we update progress.txt with score ‚Üí ralph reads score on next iteration.",
      "status": "pending"
    },
    {
      "id": "US-018",
      "title": "Cloney emotion-driven progress reporting via Qwen3 TTS",
      "description": "Cloney uses OpenWaifu's Qwen3 TTS engine to voice-report iteration progress with emotion-appropriate voice characteristics. Score changes trigger different emotional responses with matching Live2D expressions. Voice responses are concise and natural Korean.",
      "acceptanceCriteria": [
        "SSE iteration-complete events mapped to Cloney emotion + message: score <30% ‚Üí [sadness] 'Ïùå... ÏïÑÏßÅ ÎßéÏù¥ Î∂ÄÏ°±Ìï¥. Îçî Ïó¥Ïã¨Ìûà Ìï¥Î≥ºÍ≤å.', score 30-50% ‚Üí [neutral] 'Ï°∞Í∏àÏî© ÎÇòÏïÑÏßÄÍ≥† ÏûàÏñ¥. Í∏∞Îã§Î†§Ï§ò~', score 50-70% ‚Üí [joy] 'Ïò§~ Ï†êÏ†ê ÎπÑÏä∑Ìï¥ÏßÄÍ≥† ÏûàÏñ¥!', score 70-90% ‚Üí [joy] 'Í±∞Ïùò Îã§ ÏôîÏñ¥! Ï°∞Í∏àÎßå Îçî!', score >90% ‚Üí [surprise] 'ÏôÄ~! Í±∞Ïùò ÎòëÍ∞ôÏïÑ! ÌÅ¥Î°† ÏôÑÏÑ±Ïù¥Ïïº! üéâ'",
        "Score drop (regression) ‚Üí [fear] 'Ïñ¥... Ïù¥Ï†ÑÎ≥¥Îã§ ÎÇòÎπ†Ï°åÏñ¥. Îã§Ïãú ÏãúÎèÑÌï¥Î≥ºÍ≤å.'",
        "Messages sent to OpenWaifu WebSocket as text-input from system (not user)",
        "Live2D expressions change to match emotion tag: joy‚Üíexpression 3 or 6, sadness‚Üí9, surprise‚Üí7, fear‚Üí10, neutral‚Üí11",
        "Voice responses via Qwen3 TTS: per-emotion voice characteristics (pitch, speed, breathing) from OpenWaifu config",
        "Voice responses max 2 sentences to avoid blocking",
        "Progress percentage shown in Cloney's chat bubble as styled badge",
        "On loop-complete: Cloney plays celebration animation (custom motion if available, else joy expression loop)",
        "On loop-error: Cloney shows concern expression and explains error in natural language",
        "User can mute Cloney voice via toggle (chat text still appears)",
        "Typecheck passes"
      ],
      "priority": 18,
      "passes": false,
      "notes": "OpenWaifu emotion tags: [joy], [sadness], [surprise], [fear], [anger], [disgust], [smirk], [neutral]. WaifuClaw emotionMap for expression indices. Qwen3 TTS Sohee voice with per-emotion pitch/speed/breathing from qwen3_tts.py config.",
      "status": "pending"
    },
    {
      "id": "US-019",
      "title": "Dark mode default + light mode toggle + UI polish",
      "description": "Dark mode as default theme. Light/dark toggle in navigation bar. Responsive layout. Loading states, error boundaries, smooth transitions. Production-ready visual polish across all components including Live2D Cloney panel.",
      "acceptanceCriteria": [
        "Dark mode is default (class 'dark' on <html>)",
        "Toggle button in nav bar: moon icon (dark) / sun icon (light)",
        "Theme preference saved to localStorage, applied on mount before first paint (no flash)",
        "All components properly styled in both themes: backgrounds, text, borders, shadows",
        "Live2D Cloney panel background adapts to theme",
        "Chart colors visible in both themes",
        "Loading states: skeleton loaders for timeline cards, spinner for active iteration",
        "Error boundaries: graceful fallback UI for component crashes",
        "Smooth transitions: 200ms for theme switch, 300ms for panel collapse, 150ms for hover states",
        "Responsive: sidebar collapses on <1024px, timeline goes full-width, Cloney becomes floating avatar",
        "Navigation bar: logo, project name (when active), score badge (when running), theme toggle, download button",
        "Favicon: custom RalphTon icon",
        "Page title updates: 'RalphTon ‚Äî {score}% | {projectName}' when running",
        "Typecheck passes"
      ],
      "priority": 19,
      "passes": false,
      "notes": "Use CSS custom properties for theme colors. Prevent FOUC: inject theme class in <script> block in index.html before React hydrates. TailwindCSS dark: variant for all components.",
      "status": "pending"
    },
    {
      "id": "US-020",
      "title": "ralph-image-analysis dependency setup and installation script",
      "description": "As a developer, I want ralph-image-analysis to be automatically installed/configured so that the backend can spawn ralph.sh without manual setup.",
      "acceptanceCriteria": [
        "setup.sh script at project root that: clones ralph-image-analysis if not present, installs its dependencies, and verifies ralph.sh is executable",
        "ralph-image-analysis cloned to ./deps/ralph-image-analysis/ (gitignored)",
        "setup.sh installs ralph-image-analysis npm dependencies if any (npm install in ralph dir)",
        "setup.sh verifies codex CLI is available (omx mode requirement)",
        "setup.sh copies visual-verdict skill to appropriate location if needed",
        "package.json postinstall or setup script references setup.sh",
        "Environment variables documented: OPENAI_API_KEY (required for codex/vision), RALPH_MAX_SESSIONS (optional, default 3)",
        "README.md updated with full setup instructions including ralph-image-analysis dependency",
        "Backend server startup checks ralph.sh path exists and is executable, logs clear error if missing",
        ".gitignore updated to exclude deps/ralph-image-analysis/ (cloned dependency)",
        "Docker-compatible: setup.sh works in both local dev and container environments",
        "Typecheck passes"
      ],
      "passes": false,
      "notes": "ralph-image-analysis is ALREADY included at deps/ralph-image-analysis/ (no need to clone). setup.sh just needs to verify it exists, install its npm deps if any, and check omx CLI availability.",
      "status": "pending"
    },
    {
      "id": "US-021",
      "title": "OpenWaifu dependency setup and installation script",
      "description": "As a developer, I want OpenWaifu (Open-LLM-VTuber emotion voice + Live2D model pack) to be automatically installed/configured so that the frontend can connect to the OpenWaifu WebSocket server for Cloney's Live2D rendering, TTS, and conversational interface.",
      "acceptanceCriteria": [
        "setup.sh extended to: clone OpenWaifu if not present, verify install.sh is executable",
        "OpenWaifu cloned to ./deps/OpenWaifu/ (gitignored)",
        "setup.sh detects if Open-LLM-VTuber is installed at configurable path (default ./deps/Open-LLM-VTuber/)",
        "If Open-LLM-VTuber not found: clones it to ./deps/Open-LLM-VTuber/ and runs its setup",
        "Runs OpenWaifu install.sh pointing to the Open-LLM-VTuber installation path",
        "Verifies WaifuClaw Live2D model files are copied correctly (model_dict.json, live2d-models/)",
        "Verifies qwen3_tts.py is installed in Open-LLM-VTuber's TTS directory",
        "Verifies conf.yaml is applied with Cloney persona settings",
        "Environment variables documented: DASHSCOPE_API_KEY (required for Qwen3 TTS), OPENWAIFU_WS_URL (optional, default ws://localhost:12393/ws)",
        "README.md updated with OpenWaifu setup instructions including DASHSCOPE_API_KEY",
        "Backend/frontend startup checks OpenWaifu WebSocket endpoint availability, logs warning if unreachable",
        ".gitignore updated to exclude deps/OpenWaifu/ and deps/Open-LLM-VTuber/",
        "Docker-compatible: setup.sh works in both local dev and container environments",
        "package.json script 'setup:waifu' for standalone OpenWaifu setup",
        "Typecheck passes"
      ],
      "passes": false,
      "notes": "OpenWaifu is ALREADY included at deps/OpenWaifu/ (no need to clone). setup.sh just needs to verify it exists, check install.sh is executable, and document DASHSCOPE_API_KEY. Open-LLM-VTuber still needs to be cloned separately if not present.",
      "status": "pending"
    }
  ]
}
