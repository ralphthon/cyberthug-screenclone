# RalphTon (ScreenClone)

Clone any website from a screenshot using AI-powered iterative refinement with a Live2D conversational interface.

## How it works

1. **Drop a screenshot** of any website
2. **Cloney** (Live2D AI assistant powered by OpenWaifu) orchestrates the cloning process
3. **Ralph loop** iteratively generates HTML/CSS/JS using OMX coding agent, renders via Puppeteer, and evaluates using Vision AI (GPT-5.3 Codex)
4. **Watch it converge** in real-time until the clone matches the original (target: 90%+ similarity)

## Project Structure

```
screenclone/
├── scripts/ralph/          # Ralph loop config & scripts
│   ├── prd.json            # Product Requirements (21 user stories)
│   ├── ralph.sh            # Main ralph loop runner
│   ├── prompt.md           # Ralph system prompt
│   └── skills/             # Ralph skills (prd, visual-verdict)
├── deps/
│   ├── OpenWaifu/          # Live2D + Qwen3 TTS + persona
│   └── ralph-image-analysis/ # AI iterative code generation engine
├── setup.sh                # Environment setup & validation
└── README.md
```

## Stack

- **AI Engine:** ralph-image-analysis (OMX/Codex coding agent loop)
- **Visual Feedback:** Vision API (GPT-5.3 Codex) + pixelmatch diff
- **Character:** OpenWaifu (Qwen3 TTS + WaifuClaw Live2D)
- **Frontend:** React 18 + TypeScript + TailwindCSS + Vite (generated by Ralph)
- **Backend:** Express + Puppeteer + Vision API (generated by Ralph)

## Quick Start

```bash
# 1. Setup
./setup.sh

# 2. Set API keys
export OPENAI_API_KEY="your-openai-key"
export DASHSCOPE_API_KEY="your-dashscope-key"  # for Qwen3 TTS

# 3. Run ralph loop (generates the full app)
cd scripts/ralph
./ralph.sh --tool omx 1000
```

## Dependencies

### [ralph-image-analysis](https://github.com/Yeachan-Heo/ralph-image-analysis)
AI-powered iterative code generation with visual verdict feedback loop. Spawns OMX/Codex agent, generates code, renders screenshots, compares with original via Vision AI, and iterates until convergence.

### [OpenWaifu](https://github.com/HaD0Yun/OpenWaifu)
Emotion-driven voice + Live2D character system built on Open-LLM-VTuber. Includes:
- **Qwen3 TTS** with per-emotion voice characteristics (8 emotions)
- **WaifuClaw** Live2D model with 16 expressions
- **Sohee persona** — soft-spoken Korean conversational character

### Environment Requirements
- Node.js 20+
- Python 3.10+ (for OpenWaifu/OLV)
- `uv` (for Open-LLM-VTuber)
- `omx` or `codex` CLI (for ralph --tool omx)
- `OPENAI_API_KEY` (vision + codex)
- `DASHSCOPE_API_KEY` (Qwen3 TTS, optional)

### System Dependencies (Linux/Ubuntu)
```bash
# Puppeteer requires these:
sudo apt-get install -y libnss3 libatk-bridge2.0-0 libdrm2 libgbm1 libasound2
```

## PRD

Full product requirements with 21 user stories are in `scripts/ralph/prd.json`. The ralph loop reads this PRD and implements each user story sequentially.
